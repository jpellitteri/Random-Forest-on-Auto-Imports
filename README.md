# Random-Forest-on-Auto-Imports
# Task
  How well can we predict price of an automobile based on 25 variables from 1985 Auto Import Dataset with Random Forest Regression.  The data consists of 205 records and 26 variables, both continuous and categorical, including characteristics of automobiles along with some insurance risk variables.  The machine learning method being used is the Random Forest Regressor in Python.  
# Algorithm
  Random Forest Regression is an ensemble machine learning method designed to reduce the error that a singular Decision Tree typically has with prediction.  The logic behind Random Forest models is that Decision Trees are simple, easy to understand, relatively good at making predictions, but tend to have error with predicting new data (overfitting training data).  Random Forest models use many randomly created decision trees, 100 for example, and average out the regression prediction amoung all of the trees, thus averaging out the error and increasing prediction accuracy (bagging).  
  First, the data is split into training data and test data (150 records to training data, 55 records to test data). Scaling the data is not necessary as Random Forests can work with variables of unequal scale.  The model is built using 100 decision trees.  Each tree is randomized with random samples and random features at each node.  The algorithm chooses the feature that has the best split from a subset of features offered (maximim of 5 in this exercise).  This randomizes each decision tree.  Once all 100 decision trees run, the result is aggregated or averaged in this case of the continuous target variable.  
# Results
  The R^2 score is 0.93.  This is the coefficient of determination which measures how well the variance of the 25 predictor variables explain the variance of the target variable (price).  1 is a perfect score.  This says that the Random Forest Regression model is a good model for predicting the target variable with the predictor variables.  
  Random Forests have shown to be more effective at prediction of regressiona and classification over single Decision Trees. There are however times a Random Forest model is not appropriate.  If a fast model with easy visuals are important, a Decision Tree might be a better option.  Random Forests tend take up a lot of memory and computational time to train the data when the datasets get very large.
